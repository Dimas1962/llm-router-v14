# Unified Router v2.0.0 - Production-Ready LLM Routing System ðŸš€

<div align="center">

![Version](https://img.shields.io/badge/version-2.0.0-blue.svg)
![Tests](https://img.shields.io/badge/tests-455%2F457%20passed-brightgreen.svg)
![Coverage](https://img.shields.io/badge/coverage-99.6%25-brightgreen.svg)
![Status](https://img.shields.io/badge/status-production%20ready-success.svg)

**Complete LLM routing system integrating 21 components with 4 routing strategies**

</div>

## Status: Production Ready

**Implementation**: Complete integration of v1.4 + v2.0 (21 components)
**Tests**: 455/457 passed (99.6%) âœ…
**Performance**: 7.84 QPS @ 10 concurrent, 0.128s latency âœ…
**Documentation**: 2,584 lines of production-grade docs âœ…

---

## ðŸŽ¯ Overview

Unified Router v2.0 is a production-ready LLM routing system that intelligently selects the optimal model for each request based on quality, cost, and performance requirements. It seamlessly integrates 21 components from v1.4 and v2.0 into a unified 7-step pipeline.

### Key Highlights

- **21 Integrated Components**: v1.4 (8) + v2.0 (10) + Supporting (3)
- **4 Routing Strategies**: QUALITY_FOCUSED, COST_AWARE, CASCADE, BALANCED
- **7-Step Pipeline**: Runtime â†’ Sizing â†’ Assembly â†’ Routing â†’ Execution â†’ Quality â†’ Snapshot
- **High Performance**: 7.84 QPS @ 10 concurrent, 0.128s latency, 80%+ cache hit

## Quick Start

```bash
# 1. Clone and setup
git clone https://github.com/Dimas1962/llm-router-v14.git
cd llm-router-v14
python3 -m venv venv
source venv/bin/activate

# 2. Install dependencies
pip install -r requirements.txt
pip install -e .

# 3. Run tests
pytest tests/ -v

# 4. Use the router
python -c "from src.unified.unified_router import UnifiedRouter; print('âœ“ Ready')"
```

## Basic Usage

```python
import asyncio
from src.unified.unified_router import UnifiedRouter, UnifiedRequest, RoutingStrategy

async def main():
    # Initialize router
    router = UnifiedRouter(
        enable_batching=True,
        enable_quality_check=True,
        enable_monitoring=True
    )

    # Create request
    request = UnifiedRequest(
        query="Explain machine learning",
        strategy=RoutingStrategy.BALANCED,
        user_id="user123"
    )

    # Route request
    response = await router.route(request)

    # Print results
    print(f"Model: {response.model}")
    print(f"Quality: {response.quality_score:.2f}")
    print(f"Latency: {response.latency:.3f}s")
    print(f"Passed Quality Check: {response.passed_quality_check}")

asyncio.run(main())
```

## Docker Deployment

```bash
# Build and run
docker build -t unified-router:v2.0 .
docker-compose up -d

# Check health
curl http://localhost:8000/health
```

---

## Architecture

### 5 MLX Models Configured

| Model | Context | Speed | Quality | Tier | Use Cases |
|-------|---------|-------|---------|------|-----------|
| **glm-4-9b** | 1M | 45 tok/s | 0.87 | Primary | Coding, large context |
| **qwen3-next-80b** | 200K | 12 tok/s | 0.90 | Reasoning | Architecture, thinking |
| **qwen3-coder-30b** | 128K | 22 tok/s | 0.80 | Specialist | Multi-file refactoring |
| **deepseek-coder-16b** | 64K | 32 tok/s | 0.77-0.90 | Multilang | Rust, Go, Kotlin, etc. |
| **qwen2.5-coder-7b** | 32K | 60 tok/s | 0.70 | Fast | Quick snippets |

### Routing Decision Tree

```
Input Query
    â”‚
    â”œâ”€â†’ Complexity < 0.3 & Simple Pattern?
    â”‚   â””â”€â†’ YES: qwen2.5-coder-7b (cascade)
    â”‚
    â”œâ”€â†’ Requires Reasoning? (complexity > 0.7)
    â”‚   â””â”€â†’ YES: qwen3-next-80b
    â”‚
    â”œâ”€â†’ Language in [Rust, Go, Kotlin]?
    â”‚   â””â”€â†’ YES: deepseek-coder-16b
    â”‚
    â”œâ”€â†’ Context Size > 200K?
    â”‚   â””â”€â†’ YES: glm-4-9b (1M context)
    â”‚
    â”œâ”€â†’ Decay Risk > 0.7?
    â”‚   â””â”€â†’ YES: glm-4-9b or qwen3-next-80b
    â”‚
    â””â”€â†’ DEFAULT: ELO-based scoring
```

---

## Features Implemented

### âœ… Phase 1 (Complete)
- [x] Model configuration system
- [x] Task classification (10 types)
- [x] Complexity estimation
- [x] Language detection
- [x] Context analysis
- [x] Decay risk detection
- [x] Basic routing logic
- [x] Comprehensive test suite

### ðŸ”œ Future Phases
- [ ] **Phase 2**: Eagle ELO + Associative Memory
- [ ] **Phase 3**: CARROT Cost-Quality Prediction
- [ ] **Phase 4**: Advanced Context Management
- [ ] **Phase 5**: Integration & API Endpoints
- [ ] **Phase 6**: Episodic Memory & Cascade Optimization

---

## Test Results

```
âœ… 41/41 tests passed
âš¡ Routing latency: <100ms (target met)
ðŸŽ¯ Coverage: Models, classifiers, routing, context analysis
```

### Test Categories
- Model configuration validation
- Task classification accuracy
- Complexity estimation
- Language detection
- Routing decision logic
- Context analysis & decay risk
- Performance benchmarks
- Async support

---

## Project Structure

```
llm-router-v14/
â”œâ”€â”€ router/                 # Core router package
â”‚   â”œâ”€â”€ __init__.py        # Package exports
â”‚   â”œâ”€â”€ models.py          # Model configurations + ELO
â”‚   â”œâ”€â”€ classifiers.py     # Task classification + complexity
â”‚   â””â”€â”€ router_core.py     # Main RouterCore class
â”œâ”€â”€ tests/                  # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_core.py       # 41 comprehensive tests
â”œâ”€â”€ venv/                   # Virtual environment
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ SPEC.md                 # Full specification
â””â”€â”€ README.md               # This file
```

---

## API Reference

### RouterCore

Main routing class with sync/async support.

**Methods:**
- `route(query, session_history, budget, user_id)` - Async routing
- `route_sync(query, session_history, budget, user_id)` - Sync routing
- `get_model_info(model_id)` - Get model details
- `list_models()` - List all available models

### RoutingResult

Result object with routing decision.

**Attributes:**
- `model: str` - Selected model ID
- `confidence: float` - Confidence score (0.0-1.0)
- `reasoning: str` - Human-readable explanation
- `alternatives: List[Tuple[str, float]]` - Alternative models
- `metadata: Dict` - Additional routing info

---

## Performance

- **Routing Latency**: <100ms (Phase 1 target met)
- **Accuracy**: High confidence in specialized routing (language, reasoning)
- **Memory**: Minimal overhead (no ML models loaded)

---

## Next Steps

1. **Integrate with LM Studio** - Connect to actual MLX models
2. **Implement Phase 2** - Eagle ELO system with learning
3. **Add CARROT** - Cost-aware routing with budget constraints
4. **Build API** - FastAPI endpoints for production use
5. **Add Monitoring** - Track routing decisions and model performance

---

## Contributing

Phase 1 is complete and ready for testing. Future phases will add:
- Training-free learning (Eagle ELO)
- Cost optimization (CARROT)
- User personalization (Episodic memory)
- Advanced cascade routing

---

## License

Internal project - Mac Studio M4 Max setup

**Hardware**: Mac Studio M4 Max (128GB RAM)
**Models**: 5 MLX models (113.99GB total)
**Version**: 1.4.0
**Status**: Phase 1 Complete âœ…
